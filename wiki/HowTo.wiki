#summary Usage information.

== Multi-Dimensional Space Specification ==

Before doing anything useful the library needs to know the specification of the multi-dimensional space, that is, how many dimensions there are, and also the precision (number of bits) for each dimension. All this information is captured in a *!MultiDimensionalSpec* object. For compact Hilbert index and inverse calculation there are no limitations on the number of bits. For query building, the total precision must not exceed 62. In general it's a good idea to keep the precision of each dimension as small as possible.

== Fixed Size Bit Vector ==

The coordinates of a point in the multi-dimensional space need to be first represented as a `BitVector[]`. If the coordinates are of type `long` or `BitSet`, the corresponding bit vectors can be created using `BitVectorFactories.OPTIMAL.apply(precision)`, and then using the `copyFrom(long/BitSet)` overloads. There is no direct support for other numerical types such as `BigInteger` right now, so they will have to be converted to `BitSet` first.

To transform from a bit vector back into `long` use `toExactLong`; for `BitSet` use `toBitSet`.

== Compact Hilbert Index Mappings ==

For performance reasons some methods need a pre-allocated output parameter to be passed. To compute an index, call `SpaceFillingCurve.index(point, 0, index)` with a pre-allocated index bit vector of size `MultiDimensionalSpec.sumBitsPerDimension()`.

To extract the coordinates back from an index, use `SpaceFillingCurve.index(index, point)` with a pre-allocated point array.

== Queries ==

To use a multi-dimensional index in a relational database, one needs to store the (big-endian) multi-dimensional index either as the primary key (recommended), or as an indexed column in the table. Then there are a few options to consider:
  * The original coordinates do not necessarily need to be stored in the table. However if the number of ranges and optionally sub-ranges (more generally, filters combined through a *!FilterCombiner*) generated for queries tends to be too high, it might be useful to either limit, or not use the sub-ranges feature at all, besides limiting the number of high level ranges, and append to the where clause of the SQL query further filtering of the points to be selected based on their coordinates.
  * There are a few ways to construct the main part of the SQL queries, and which one to use can probably be decided only through experimentation:
    * Some databases such as H2 have special support for array-type prepared statement parameters, in which case redundantly storing the coordinates as separate columns and using the TABLE feature probably works best (see H2's *!TestMultiDimension* and *!MultiDimension* classes).
    * Some databases might combine multiple UNION ALL statements into one during query optimisation, and if the number of ranges is high then do a full-table scan, even though only a small subset of the table is selected. In such a database the coordinates should be stored redundantly and the sub-ranges feature should not be used at all, i.e. use *!PlainFilterCombiner* as the filter combiner and `PlainFilterCombiner.FILTER` as the unique filter object).
    * For the other databases it might make sense to create a UNION ALL of the main ranges, and express each range as a where clause consisting in a set of sub-ranges, i.e. use *!ListConcatCombiner* as the filter combiner.

In either case the query region is represented by the *!RegionInspector* abstraction, with the simple *!SimpleRegionInspector* implementation provided out of the box for queries consisting of one or more hyper-rectangles. Note that the query doesn't have to look like a (small) set of hyper-rectangles, and can in general have any shape, with the inherent performance price and the need to implement a custom region inspector.